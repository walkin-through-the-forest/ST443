{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST 443 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "For small $k$ the variance is high because the predictor $\\hat{f}$ relies only on a few points around $x_i$. Thus, each of these points individually has a higher influence on the predictor, greatly increasing the influence of outliers and increasing the variance. The bias is lower because local effects can be captured more precisely, as the predictor is not influenced by \"far-away\" points.\n",
    "\n",
    "For large $k$, the opposite is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "For 1-nearest neighbours, the training error is zero because $\\hat{f}(x_i) = x_i$; i.e. the predictor predicts exactly the observed $y$ value for any input $x$ in the training data (assuming all $x_i$ are unique). The test error must then be $36\\%$ to get an average error rate of $18\\%$. I would prefer to use logistic regression because it has a lower test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "For any $X < 0$ and $X > 1$, the Bayes error rate is zero because the classification is deterministic\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "    1 & x < 0 \\\\\n",
    "    0 & x > 1\n",
    "\\end{cases}$\n",
    "\n",
    "For $0 \\leq x \\leq 1$, let's compute $p(y|X=x) \\propto f(x|Y=y)p(y)$\n",
    "\n",
    "$p(y) = \\begin{cases}\n",
    "    1/3 & y = 0 \\\\\n",
    "    2/3 & y = 1\n",
    "\\end{cases}$\n",
    "\n",
    "$f(x|Y=y) = \\begin{cases}\n",
    "    1/4 & y = 0 \\\\\n",
    "    1/3 & y = 1\n",
    "\\end{cases}$\n",
    "\n",
    "$p(y|X=x) = \\begin{cases}\n",
    "    1/12 & y = 0 \\\\\n",
    "    2/9 & y = 1\n",
    "\\end{cases}$\n",
    "\n",
    "so points $0 \\leq x \\leq 1$ are classified as $Y = 1$. The probability of misclassification is $(1/12)/(1/12 + 2/9) = 3/11$. The probability of $X$ falling in this range is\n",
    "$$\\mathbb{P}[0 \\leq x \\leq 1] = \\mathbb{P}[0 \\leq x \\leq 1 | Y = 0]\\mathbb{P}[Y = 0] + \\mathbb{P}[0 \\leq x \\leq 1 | Y = 1]\\mathbb{P}[Y = 1] = (1/4)(1/3) + (1/3)(2/3) = 11/36$$\n",
    "The unavoidable error is the product of probability of $X$ being in this range times the probability of misclassification in this range\n",
    "$$\\boxed{\\mathbb{E}\\varepsilon(X) = 1/12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "### Part a\n",
    "The Bayes classifier is\n",
    "$\\psi(x) = \\begin{cases}\n",
    "    0 & 1/4 \\leq x \\leq 3/4 \\\\\n",
    "    1 & \\textrm{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "### Part b\n",
    "$$R^* = \\mathbb{E} 1[\\psi(X) \\neq Y] = \\mathbb{E}_X \\mathbb{E}_{Y|X} [1[\\psi(X) \\neq Y] | X = x] = \\int_0^1 \\sum_{y=0,1} 1[\\psi(x) \\neq y] p(y|X=x) dx$$\n",
    "$$ = \\int_0^1(1[\\psi(x)=0]*p(1|X=x) + 1[\\psi(x)=1]*p(0|X=x)) dx$$\n",
    "$$ = \\int_{1/4}^{3/4} 2|x-1/2| dx + \\left(\\int_0^{1/4} + \\int_{3/4}^1\\right) (1-2|x-1/2|) dx$$\n",
    "$$\\boxed{R^* = 1/4}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "By Bayes's rule, $p(y|X=x) \\propto f(x|Y=y) p(y)$. We are given $p(y)$ and $f(x|Y=y)$\n",
    "\n",
    "$p(y) = \\begin{cases}\n",
    "    2/5 & y = 1 \\\\\n",
    "    2/5 & y = 2 \\\\\n",
    "    1/5 & y = 3\n",
    "\\end{cases}$\n",
    "\n",
    "$f(x|Y=y) = \\begin{cases}\n",
    "    f_{N(\\mu_1, \\Sigma)}(x) & y = 1 \\\\\n",
    "    f_{N(\\mu_2, \\Sigma)}(x) & y = 2 \\\\\n",
    "    0.5 f_{N(\\mu_{31}, \\Sigma)}(x) + 0.5 f_{N(\\mu_{32}, \\Sigma)}(x) & y = 3\n",
    "\\end{cases}$\n",
    "\n",
    "Multiplying these two gives\n",
    "\n",
    "$p(y|X=x) \\propto \\begin{cases}\n",
    "    0.4 f_{N(\\mu_1, \\Sigma)}(x) & y = 1 \\\\\n",
    "    0.4 f_{N(\\mu_2, \\Sigma)}(x) & y = 2 \\\\\n",
    "    0.1 f_{N(\\mu_{31}, \\Sigma)}(x) + 0.1 f_{N(\\mu_{32}, \\Sigma)}(x) & y = 3\n",
    "\\end{cases}$\n",
    "\n",
    "Evaluating these at $x_0 = (0.3, 0.3)^T$ with $f_{N(\\mu_1, \\Sigma)}(x_0) = 0.145$, $f_{N(\\mu_2, \\Sigma)}(x_0) = 0.0975$, $f_{N(\\mu_{31}, \\Sigma)}(x_0) = 0.153$, and $f_{N(\\mu_{32}, \\Sigma)}(x_0) = 0.1133$ gives\n",
    "\n",
    "$p(y|X=x_0) = \\begin{cases}\n",
    "    0.0581 & y = 1 \\\\\n",
    "    0.0390 & y = 2 \\\\\n",
    "    0.0266 & y = 3\n",
    "\\end{cases}$\n",
    "\n",
    "Based on the posterior probabilities, $x_0$ would be classified as $Y = 1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
